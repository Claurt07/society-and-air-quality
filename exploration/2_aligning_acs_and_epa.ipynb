{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37464bitanaconda3virtualenv214083d9e99e48dbbb65188af8bf868a",
   "display_name": "Python 3.7.4 64-bit ('anaconda3': virtualenv)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Aligning ACS and EPA Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our historical air quality data from the EPA is provided station by station, and the positions of each station is provided by coordinates. To make useful comparisons, we want to align the data - decide for each year which exact air quality data points align with each CBSA GeoID/region in ACS.\n",
    "\n",
    "We'll refer to these as `epa_point` and `acs_region` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "env: GOOGLE_APPLICATION_CREDENTIALS=../google_app_credentials.json\n"
    }
   ],
   "source": [
    "# set up path to app credentials - see exploration/README.md\n",
    "%env GOOGLE_APPLICATION_CREDENTIALS=../google_app_credentials.json\n",
    "\n",
    "# set up bigquery client\n",
    "from google.cloud import bigquery\n",
    "bq = bigquery.Client()\n",
    "\n",
    "# set up some parameters\n",
    "BQ_DATASET_ID = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up some dependencies\n",
    "import json, time, random\n",
    "\n",
    "import geopandas as gp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import descartes # for plotting with geopanadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll load up the relevant data, which should already be set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# set up acs utils\n",
    "resp = bq.query('''\n",
    "    SELECT DISTINCT do_date AS year\n",
    "    FROM `eosc410-project.data.acs_cbsa_20*`\n",
    "    ORDER BY do_date ASC\n",
    "''')\n",
    "acs_years = [row[\"year\"] for row in resp]\n",
    "\n",
    "def load_geojson(y) -> gp.GeoDataFrame:\n",
    "    print('=> loading %s' % y)\n",
    "    geo = gp.read_file('../_data/tmp/acs_cbsa_%s/geojson.json' % y)\n",
    "    return geo\n",
    "\n",
    "def get_acs_year(i: int) -> str:\n",
    "    return str(int(acs_years[0]) + i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "=> loading 2007\n2007 has 952 features (regions)\nCSAFP                                                        \nCBSAFP                                                  27980\nNAME                                      Kahului-Wailuku, HI\nNAMELSAD    Kahului-Wailuku, HI Micropolitan Statistical Area\nLSAD                                                       M2\nMEMI                                                        2\nMTFCC                                                   G3110\nFUNCSTAT                                                    S\ngeometry    (POLYGON ((-156.705406 20.82632, -156.711227 2...\nName: 0, dtype: object\n2007 27980: Kahului-Wailuku, HI\n{\"type\": \"MultiPolygon\", \"coordinates\": [[[[-156.70540599999998, 20.82632], [-156.711227, 20.832228]...\n"
    }
   ],
   "source": [
    "# look at what our geojson looks like\n",
    "test_year = '2007'\n",
    "test_region_geojson = load_geojson(test_year)\n",
    "print('%s has %d features (regions)' % (test_year, len(test_region_geojson)))\n",
    "\n",
    "test_region = test_region_geojson.loc[0] # first region in this year\n",
    "print(test_region)\n",
    "test_region_id = test_region['CBSAFP']   # current metropolitan statistical area/micropolitan statistical area code\n",
    "                                         # see https://www2.census.gov/geo/pdfs/maps-data/data/tiger/tgrshp2014/TGRSHP2014_TechDoc_Ch3.pdf\n",
    "test_region_name = test_region['NAME']   # this region has a name\n",
    "test_region_geo = test_region.geometry   # this region has a geometry\n",
    "print('%s %s: %s' % (test_year, test_region_id, test_region_name))\n",
    "print(json.dumps(test_region_geo.__geo_interface__)[:100] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Stations in region 27980 \"Kahului-Wailuku, HI\" in 2007:\nsite 9001 (Haleakala National Park, Kahului-Wailuku-Lahaina, HI)\nsite 0006 (KAIHOI ST AND KAIOLOHIA ST, Kahului-Wailuku-Lahaina, HI)\nsite 9000 (Haleakala National Park, HI 96768, Kahului-Wailuku-Lahaina, HI)\n"
    }
   ],
   "source": [
    "# for each year and acs region, we want to get associated station numbers\n",
    "# try with the test region: find all EPA points that fall within this region's geometry in that year\n",
    "resp = bq.query('''\n",
    "SELECT DISTINCT site_num, address, cbsa_name\n",
    "FROM `eosc410-project.data.epa_air_quality_annual` as epa\n",
    "WHERE\n",
    "  ST_WITHIN(ST_GEOGPOINT(epa.longitude, epa.latitude), ST_GEOGFROMGEOJSON('%s'))\n",
    "  AND year = %s\n",
    "''' % (json.dumps(test_region_geo.__geo_interface__), test_year))\n",
    "print('Stations in region %s \"%s\" in %s:' % (test_region_id, test_region_name, test_year))\n",
    "for row in resp:\n",
    "    print('site %s (%s, %s)' % (row['site_num'], row['address'], row['cbsa_name']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As demonstrated in the proof of concept above, we can easily query for geographic traits in BigQuery. Let's load the ACS geometries into BigQuery directly, since once we compile the regions we can avoid handling massive geometry data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "'''\n",
    "A utility function to upload a year of boundaries to BigQuery\n",
    "'''\n",
    "dataset_ref = bq.dataset(BQ_DATASET_ID)\n",
    "def upload_year_to_bq(year):\n",
    "    # set up data\n",
    "    geo = load_geojson(year)\n",
    "    geoid_key = 'CBSAFP'\n",
    "    name_key = 'NAME'\n",
    "    # 2010 has... a different keys for everything\n",
    "    if year == '2010':\n",
    "        geoid_key = 'CBSAFP10'\n",
    "        name_key = 'NAME10'\n",
    "\n",
    "    # convert to CSV for ingesting into bigquery\n",
    "    table_data_path = '../_data/tmp/acs_cbsa_%s/bq.csv' % year\n",
    "    print('%s: writing data to \"%s\"' % (year, table_data_path))\n",
    "    if os.path.exists(table_data_path):\n",
    "        os.remove(table_data_path)\n",
    "    with open(table_data_path, 'w') as csvfile:\n",
    "        w = csv.DictWriter(csvfile, fieldnames=['year', 'geoid', 'name', 'geometry'])\n",
    "        w.writeheader()\n",
    "        # let each row represent a region\n",
    "        for i in range(0, len(geo)):\n",
    "            region = geo.loc[i]\n",
    "            geoid = region[geoid_key]\n",
    "            name = region[name_key]\n",
    "            geometry = region.geometry.__geo_interface__\n",
    "            w.writerow({'year': year, 'geoid': geoid, 'name': name, 'geometry': json.dumps(geometry)})\n",
    "\n",
    "    # move to bigquery\n",
    "    table_ref = dataset_ref.table('acs_cbsa_boundaries_%s' % year)\n",
    "    print('%s: setting up table \"%s\"' % (year, table_ref))\n",
    "    bq.delete_table(table_ref, not_found_ok=True)\n",
    "    bq.create_table(table_ref)\n",
    "    job_config = bigquery.LoadJobConfig()\n",
    "    job_config.source_format = bigquery.SourceFormat.CSV\n",
    "    job_config.autodetect = True\n",
    "    with open(table_data_path, \"rb\") as source_file:\n",
    "        print('%s: uploading...' % year)\n",
    "        job = bq.load_table_from_file(source_file, table_ref, job_config=job_config)\n",
    "        print('%s: done' % year)\n",
    "    try:\n",
    "        job.result() # wait for upload to complete\n",
    "    except:\n",
    "        for err in job.errors:\n",
    "            print(err)\n",
    "    return table_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "=> loading 2007\n2007: writing data to \"../_data/tmp/acs_cbsa_2007/bq.csv\"\n2007: setting up table \"TableReference(DatasetReference('eosc410-project', 'data'), 'acs_cbsa_boundaries_2007')\"\n2007: uploading...\n2007: done\n=> loading 2008\n2008: writing data to \"../_data/tmp/acs_cbsa_2008/bq.csv\"\n2008: setting up table \"TableReference(DatasetReference('eosc410-project', 'data'), 'acs_cbsa_boundaries_2008')\"\n2008: uploading...\n2008: done\n=> loading 2009\n2009: writing data to \"../_data/tmp/acs_cbsa_2009/bq.csv\"\n2009: setting up table \"TableReference(DatasetReference('eosc410-project', 'data'), 'acs_cbsa_boundaries_2009')\"\n2009: uploading...\n2009: done\n=> loading 2010\n2010: writing data to \"../_data/tmp/acs_cbsa_2010/bq.csv\"\n2010: setting up table \"TableReference(DatasetReference('eosc410-project', 'data'), 'acs_cbsa_boundaries_2010')\"\n2010: uploading...\n2010: done\n=> loading 2011\n2011: writing data to \"../_data/tmp/acs_cbsa_2011/bq.csv\"\n2011: setting up table \"TableReference(DatasetReference('eosc410-project', 'data'), 'acs_cbsa_boundaries_2011')\"\n2011: uploading...\n2011: done\n=> loading 2012\n2012: writing data to \"../_data/tmp/acs_cbsa_2012/bq.csv\"\n2012: setting up table \"TableReference(DatasetReference('eosc410-project', 'data'), 'acs_cbsa_boundaries_2012')\"\n2012: uploading...\n2012: done\n=> loading 2013\n2013: writing data to \"../_data/tmp/acs_cbsa_2013/bq.csv\"\n2013: setting up table \"TableReference(DatasetReference('eosc410-project', 'data'), 'acs_cbsa_boundaries_2013')\"\n2013: uploading...\n2013: done\n=> loading 2014\n2014: writing data to \"../_data/tmp/acs_cbsa_2014/bq.csv\"\n2014: setting up table \"TableReference(DatasetReference('eosc410-project', 'data'), 'acs_cbsa_boundaries_2014')\"\n2014: uploading...\n2014: done\n=> loading 2015\n2015: writing data to \"../_data/tmp/acs_cbsa_2015/bq.csv\"\n2015: setting up table \"TableReference(DatasetReference('eosc410-project', 'data'), 'acs_cbsa_boundaries_2015')\"\n2015: uploading...\n2015: done\n=> loading 2016\n2016: writing data to \"../_data/tmp/acs_cbsa_2016/bq.csv\"\n2016: setting up table \"TableReference(DatasetReference('eosc410-project', 'data'), 'acs_cbsa_boundaries_2016')\"\n2016: uploading...\n2016: done\n=> loading 2017\n2017: writing data to \"../_data/tmp/acs_cbsa_2017/bq.csv\"\n2017: setting up table \"TableReference(DatasetReference('eosc410-project', 'data'), 'acs_cbsa_boundaries_2017')\"\n2017: uploading...\n2017: done\n=> loading 2018\n2018: writing data to \"../_data/tmp/acs_cbsa_2018/bq.csv\"\n2018: setting up table \"TableReference(DatasetReference('eosc410-project', 'data'), 'acs_cbsa_boundaries_2018')\"\n2018: uploading...\n2018: done\n"
    }
   ],
   "source": [
    "# upload all years\n",
    "for year in acs_years:\n",
    "    upload_year_to_bq(year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all our geometries available in BigQuery, let's attempt the proof-of-concept from before entirely in BigQuery:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Stations in region 27980 \"Kahului-Wailuku, HI\" in 2007:\nsite 9000 (Haleakala National Park, HI 96768, Kahului-Wailuku-Lahaina, HI)\nsite 0006 (KAIHOI ST AND KAIOLOHIA ST, Kahului-Wailuku-Lahaina, HI)\nsite 9001 (Haleakala National Park, Kahului-Wailuku-Lahaina, HI)\n"
    }
   ],
   "source": [
    "resp = bq.query('''\n",
    "    SELECT DISTINCT\n",
    "        epa.site_num, epa.address, epa.cbsa_name\n",
    "    FROM\n",
    "        `eosc410-project.data.epa_air_quality_annual` as epa,\n",
    "        `eosc410-project.data.acs_cbsa_boundaries_%s` as acs\n",
    "    WHERE\n",
    "        ST_WITHIN(ST_GEOGPOINT(epa.longitude, epa.latitude), ST_GEOGFROMGEOJSON(acs.geometry))\n",
    "        AND epa.year = %s\n",
    "        AND acs.geoid = %s\n",
    "''' % (test_year, test_year, test_region_id))\n",
    "\n",
    "print('Stations in region %s \"%s\" in %s:' % (test_region_id, test_region_name, test_year))\n",
    "for row in resp:\n",
    "    print('site %s (%s, %s)' % (row['site_num'], row['address'], row['cbsa_name']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That worked! Now for each year, dump all relationships between EPA stations and ACS regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Utility to match EPA stations with ACS regions for a given year\n",
    "'''\n",
    "def get_relations_for_year(year) -> pd.DataFrame:\n",
    "    print('%s: querying' % year)\n",
    "    start = time.time()\n",
    "    resp = bq.query('''\n",
    "        SELECT DISTINCT\n",
    "            acs.year,\n",
    "            acs.geoid AS acs_geoid,\n",
    "            acs.name AS cbsa_name,\n",
    "            epa.site_num AS epa_site,\n",
    "            epa.longitude,\n",
    "            epa.latitude\n",
    "        FROM\n",
    "            `eosc410-project.data.epa_air_quality_annual` as epa,\n",
    "            `eosc410-project.data.acs_cbsa_boundaries_%s` as acs\n",
    "        WHERE\n",
    "            epa.year = %s\n",
    "            AND ST_WITHIN(ST_GEOGPOINT(epa.longitude, epa.latitude), ST_GEOGFROMGEOJSON(acs.geometry))\n",
    "            AND (epa.datum='NAD83' OR epa.datum='WGS84') # guard against irrelevant coordinates\n",
    "    ''' % (year, year))\n",
    "    print('%s: done, took %s' % (year, time.time()-start))\n",
    "    return resp.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "2007: querying\n2007: done, took 1.4955708980560303\n      year  acs_geoid                                     cbsa_name epa_site  \\\n0     2007      17900                                  Columbia, SC     0020   \n1     2007      17900                                  Columbia, SC     1002   \n2     2007      22180                              Fayetteville, NC     0009   \n3     2007      41940            San Jose-Sunnyvale-Santa Clara, CA     0003   \n4     2007      41940            San Jose-Sunnyvale-Santa Clara, CA     0005   \n...    ...        ...                                           ...      ...   \n2680  2007      47900  Washington-Arlington-Alexandria, DC-VA-MD-WV     0004   \n2681  2007      17140               Cincinnati-Middletown, OH-KY-IN     5001   \n2682  2007      16740             Charlotte-Gastonia-Concord, NC-SC     1001   \n2683  2007      20260                                 Duluth, MN-WI     0014   \n2684  2007      37740                                    Payson, AZ     8021   \n\n       longitude   latitude  \n0     -81.034179  34.015494  \n1     -81.065326  33.969004  \n2     -78.953112  35.041416  \n3    -121.156880  36.483240  \n4    -121.894898  37.348497  \n...          ...        ...  \n2680  -77.487120  38.302250  \n2681  -84.453978  39.226729  \n2682  -80.852585  35.498433  \n2683  -92.120747  46.731330  \n2684 -110.771458  32.993040  \n\n[2685 rows x 6 columns]\n"
    }
   ],
   "source": [
    "# test one one year\n",
    "print(get_relations_for_year('2007'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "2007: querying\n2007: done, took 0.3855931758880615\n2007: dumped to ../_data/epa_to_acs_cbsa/2007.csv\n2008: querying\n2008: done, took 0.2838127613067627\n2008: dumped to ../_data/epa_to_acs_cbsa/2008.csv\n2009: querying\n2009: done, took 0.2727169990539551\n2009: dumped to ../_data/epa_to_acs_cbsa/2009.csv\n2010: querying\n2010: done, took 0.42783617973327637\n2010: dumped to ../_data/epa_to_acs_cbsa/2010.csv\n2011: querying\n2011: done, took 0.2758667469024658\n2011: dumped to ../_data/epa_to_acs_cbsa/2011.csv\n2012: querying\n2012: done, took 0.300947904586792\n2012: dumped to ../_data/epa_to_acs_cbsa/2012.csv\n2013: querying\n2013: done, took 0.2583580017089844\n2013: dumped to ../_data/epa_to_acs_cbsa/2013.csv\n2014: querying\n2014: done, took 0.30374908447265625\n2014: dumped to ../_data/epa_to_acs_cbsa/2014.csv\n2015: querying\n2015: done, took 0.31137704849243164\n2015: dumped to ../_data/epa_to_acs_cbsa/2015.csv\n2016: querying\n2016: done, took 0.28185105323791504\n2016: dumped to ../_data/epa_to_acs_cbsa/2016.csv\n2017: querying\n2017: done, took 0.28124403953552246\n2017: dumped to ../_data/epa_to_acs_cbsa/2017.csv\n2018: querying\n2018: done, took 0.29460930824279785\n2018: dumped to ../_data/epa_to_acs_cbsa/2018.csv\n"
    }
   ],
   "source": [
    "# get for all years and dump each\n",
    "for year in acs_years:\n",
    "    df = get_relations_for_year(year)\n",
    "    out = '../_data/epa_to_acs_cbsa/%s.csv' % year\n",
    "    df.to_csv(out)\n",
    "    print('%s: dumped to %s' % (year, out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at what the distribution of EPA stations looks like for 2018!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "=> loading 2018\ndone\n"
    }
   ],
   "source": [
    "# load data\n",
    "year = '2018'\n",
    "stations = pd.read_csv('../_data/epa_to_acs_cbsa/%s.csv' % year)\n",
    "regions = load_geojson(year)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 1920x1080 with 0 Axes>"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "found 408 regions without stations:\n['39700', '39940', '39980', '40100', '40260', '40300', '40460', '40620', '40780', '40820', '41900', '40940', '40980', '41060', '41220', '41660', '41780', '42180', '42740', '42780', '42460', '42620', '42820', '43020', '43060', '43140', '44860', '43180', '43300', '44920', '43380', '44940', '43460', '43500', '43660', '43940', '43980', '44020', '44260', '44340', '44420', '44500', '44540', '44580', '44620', '44660', '44740', '44780', '44980', '45000', '45020', '45180', '45380', '45540', '45580', '45620', '45660', '45700', '45740', '46100', '46460', '46500', '46620', '46740', '46780', '46820', '46900', '46980', '47080', '47420', '47540', '47660', '47700', '48100', '48180', '48460', '49780', '49820', '48660', '48820', '48980', '49080', '49220', '49260', '49300', '49380', '49460', '34660', '35140', '34860', '35020', '35060', '35100', '35420', '35440', '35460', '35500', '35580', '35740', '35820', '35860', '35900', '35940', '36020', '36140', '36340', '36380', '36460', '36580', '36620', '36660', '36700', '36780', '36820', '36840', '36900', '36940', '37020', '37060', '37120', '37300', '37940', '37420', '37500', '37540', '37580', '37660', '37780', '38100', '38220', '38240', '38260', '38380', '38460', '38500', '38700', '38740', '38780', '38920', '39060', '39460', '10180', '10220', '10380', '10460', '10620', '10660', '10700', '10820', '10860', '10940', '10980', '11060', '11180', '11220', '11380', '11420', '11500', '11580', '11680', '11740', '11820', '11860', '19580', '19620', '19760', '19860', '19940', '20140', '20220', '20300', '20340', '20380', '30420', '30580', '30660', '30820', '30880', '30900', '30940', '31260', '31300', '31380', '31500', '31580', '31620', '31680', '31740', '31860', '31900', '31940', '31980', '32000', '32100', '32180', '32260', '32280', '32300', '32340', '32420', '32460', '32500', '32620', '32660', '32700', '32740', '32860', '32980', '33020', '33260', '33300', '33420', '33580', '33620', '34020', '34220', '34260', '34300', '34340', '34380', '34460', '34500', '20660', '20780', '20900', '21020', '21120', '21180', '21300', '21380', '21460', '21540', '21840', '22060', '22260', '22300', '22340', '22580', '22620', '22700', '22780', '22820', '22860', '23140', '23180', '23240', '23300', '23380', '23500', '23620', '23660', '23700', '23780', '23980', '24020', '24100', '24140', '24220', '24380', '24460', '24620', '24700', '24740', '24820', '24900', '24940', '25100', '25300', '25580', '25700', '25720', '25760', '25780', '25820', '25880', '25940', '25980', '26220', '26460', '26500', '26660', '26700', '26740', '26780', '26940', '27020', '27100', '27160', '27300', '27340', '27380', '27420', '27580', '27600', '27700', '27740', '27920', '28100', '28260', '28340', '28380', '28500', '28540', '28580', '28620', '28740', '28860', '29300', '29500', '29780', '29860', '29900', '29940', '30060', '30220', '30260', '30280', '30380', '11980', '12140', '12180', '12220', '12380', '12460', '12680', '12740', '12820', '12860', '12900', '12980', '13060', '13100', '13220', '13260', '13300', '13340', '13500', '13660', '13720', '13780', '13940', '14100', '14140', '14180', '14220', '14340', '14380', '14620', '14720', '14780', '14820', '15020', '15060', '15220', '15340', '15420', '15460', '15500', '15660', '15680', '15740', '15780', '15820', '15900', '16020', '16060', '16340', '16380', '16460', '16660', '17060', '17260', '17420', '17500', '17580', '17620', '17640', '17700', '17740', '18060', '18100', '18220', '18300', '18380', '18420', '18460', '18660', '18740', '18820', '18900', '18980', '19180', '19220', '19260', '19420', '19540', '10760', '14300', '15140', '16140', '16420', '21640', '21860', '24330', '26260', '27660', '37800', '48500']\n"
    }
   ],
   "source": [
    "# random rgb value generator\n",
    "r = lambda: random.randint(0,255)\n",
    "\n",
    "# just good to knnow\n",
    "no_stations = []\n",
    "\n",
    "# set up plot\n",
    "print('starting')\n",
    "dpi=72\n",
    "plt.figure(figsize=(1920/dpi,1080/dpi),dpi=dpi)\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(len(regions)):\n",
    "    # check if this region has any stations\n",
    "    geoid = regions.loc[i]['CBSAFP']\n",
    "    sts = stations[stations.acs_geoid==int(geoid)]\n",
    "    if sts.size > 0:\n",
    "        # random color for this region\n",
    "        c = '#%02x%02x%02x' % (r(), r(), r())\n",
    "\n",
    "        # plot region\n",
    "        regions.loc[[i],'geometry'].plot(ax=ax, color='white', edgecolor=c)\n",
    "\n",
    "        # plot stations\n",
    "        gdf = gp.GeoDataFrame(sts, geometry=gp.points_from_xy(sts.longitude, sts.latitude))\n",
    "        gdf.plot(ax=ax, marker='o', color=c, markersize=5)\n",
    "    else:\n",
    "        print('%d: no stations in region %s, ignoring' % (i, geoid))\n",
    "        no_stations.append(geoid)\n",
    "\n",
    "print('wrapping up')\n",
    "plt.title('ACS CBSA Regions and EPA Stations in %s' % year)\n",
    "plt.tight_layout()\n",
    "plt.savefig('./figs/2/epa_stations_in_acs_regions_2018.png')\n",
    "\n",
    "print('found %d regions without stations:' % len(no_stations))\n",
    "print(no_stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}