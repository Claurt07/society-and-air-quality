{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37464bitanaconda3virtualenv214083d9e99e48dbbb65188af8bf868a",
   "display_name": "Python 3.7.4 64-bit ('anaconda3': virtualenv)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Aligning ACS and EPA Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our historical air quality data from the EPA is provided station by station, and the positions of each station is provided by coordinates. To make useful comparisons, we want to align the data - decide for each year which exact air quality data points align with each CBSA GeoID/region in ACS.\n",
    "\n",
    "We'll refer to these as `epa_point` and `acs_region` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "env: GOOGLE_APPLICATION_CREDENTIALS=../google_app_credentials.json\n"
    }
   ],
   "source": [
    "# set up path to app credentials - see exploration/README.md\n",
    "%env GOOGLE_APPLICATION_CREDENTIALS=../google_app_credentials.json\n",
    "\n",
    "# set up bigquery client\n",
    "from google.cloud import bigquery\n",
    "bq = bigquery.Client()\n",
    "\n",
    "# set up some parameters\n",
    "BQ_DATASET_ID = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up some dependencies\n",
    "import json, time, random\n",
    "\n",
    "import geopandas as gp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import descartes # for plotting with geopanadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll load up the relevant data, which should already be set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# set up acs utils\n",
    "resp = bq.query('''\n",
    "    SELECT DISTINCT do_date AS year\n",
    "    FROM `eosc410-project.data.acs_cbsa_20*`\n",
    "    ORDER BY do_date ASC\n",
    "''')\n",
    "acs_years = [row[\"year\"] for row in resp]\n",
    "\n",
    "def load_geojson(y) -> gp.GeoDataFrame:\n",
    "    print('=> loading %s' % y)\n",
    "    geo = gp.read_file('../_data/tmp/acs_cbsa_%s/geojson.json' % y)\n",
    "    return geo\n",
    "\n",
    "def get_acs_year(i: int) -> str:\n",
    "    return str(int(acs_years[0]) + i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "=> loading 2007\n2007 has 952 features (regions)\nCSAFP                                                        \nCBSAFP                                                  27980\nNAME                                      Kahului-Wailuku, HI\nNAMELSAD    Kahului-Wailuku, HI Micropolitan Statistical Area\nLSAD                                                       M2\nMEMI                                                        2\nMTFCC                                                   G3110\nFUNCSTAT                                                    S\ngeometry    (POLYGON ((-156.705406 20.82632, -156.711227 2...\nName: 0, dtype: object\n2007 27980: Kahului-Wailuku, HI\n{\"type\": \"MultiPolygon\", \"coordinates\": [[[[-156.70540599999998, 20.82632], [-156.711227, 20.832228]...\n"
    }
   ],
   "source": [
    "# look at what our geojson looks like\n",
    "test_year = '2007'\n",
    "test_region_geojson = load_geojson(test_year)\n",
    "print('%s has %d features (regions)' % (test_year, len(test_region_geojson)))\n",
    "\n",
    "test_region = test_region_geojson.loc[0] # first region in this year\n",
    "print(test_region)\n",
    "test_region_id = test_region['CBSAFP']   # current metropolitan statistical area/micropolitan statistical area code\n",
    "                                         # see https://www2.census.gov/geo/pdfs/maps-data/data/tiger/tgrshp2014/TGRSHP2014_TechDoc_Ch3.pdf\n",
    "test_region_name = test_region['NAME']   # this region has a name\n",
    "test_region_geo = test_region.geometry   # this region has a geometry\n",
    "print('%s %s: %s' % (test_year, test_region_id, test_region_name))\n",
    "print(json.dumps(test_region_geo.__geo_interface__)[:100] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Stations in region 27980 \"Kahului-Wailuku, HI\" in 2007:\nsite 15.009.9001 (Haleakala National Park, Kahului-Wailuku-Lahaina, HI)\nsite 15.009.0006 (KAIHOI ST AND KAIOLOHIA ST, Kahului-Wailuku-Lahaina, HI)\nsite 15.009.9000 (Haleakala National Park, HI 96768, Kahului-Wailuku-Lahaina, HI)\n"
    }
   ],
   "source": [
    "# for each year and acs region, we want to get associated station numbers\n",
    "# try with the test region: find all EPA points that fall within this region's geometry in that year\n",
    "resp = bq.query('''\n",
    "SELECT DISTINCT state_code, county_code, site_num, address, cbsa_name\n",
    "FROM `eosc410-project.data.epa_air_quality_annual` as epa\n",
    "WHERE\n",
    "  ST_WITHIN(ST_GEOGPOINT(epa.longitude, epa.latitude), ST_GEOGFROMGEOJSON('%s'))\n",
    "  AND year = %s\n",
    "''' % (json.dumps(test_region_geo.__geo_interface__), test_year))\n",
    "print('Stations in region %s \"%s\" in %s:' % (test_region_id, test_region_name, test_year))\n",
    "for row in resp:\n",
    "    print('site %s.%s.%s (%s, %s)' % (row['state_code'], row['county_code'], row['site_num'], row['address'], row['cbsa_name']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As demonstrated in the proof of concept above, we can easily query for geographic traits in BigQuery. Let's load the ACS geometries into BigQuery directly, since once we compile the regions we can avoid handling massive geometry data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "'''\n",
    "A utility function to upload a year of boundaries to BigQuery\n",
    "'''\n",
    "dataset_ref = bq.dataset(BQ_DATASET_ID)\n",
    "def upload_year_to_bq(year):\n",
    "    # set up data\n",
    "    geo = load_geojson(year)\n",
    "    geoid_key = 'CBSAFP'\n",
    "    name_key = 'NAME'\n",
    "    # 2010 has... a different keys for everything\n",
    "    if year == '2010':\n",
    "        geoid_key = 'CBSAFP10'\n",
    "        name_key = 'NAME10'\n",
    "\n",
    "    # convert to CSV for ingesting into bigquery\n",
    "    table_data_path = '../_data/tmp/acs_cbsa_%s/bq.csv' % year\n",
    "    print('%s: writing data to \"%s\"' % (year, table_data_path))\n",
    "    if os.path.exists(table_data_path):\n",
    "        os.remove(table_data_path)\n",
    "    with open(table_data_path, 'w') as csvfile:\n",
    "        w = csv.DictWriter(csvfile, fieldnames=['year', 'geoid', 'name', 'geometry'])\n",
    "        w.writeheader()\n",
    "        # let each row represent a region\n",
    "        for i in range(0, len(geo)):\n",
    "            region = geo.loc[i]\n",
    "            geoid = region[geoid_key]\n",
    "            name = region[name_key]\n",
    "            geometry = region.geometry.__geo_interface__\n",
    "            w.writerow({'year': year, 'geoid': geoid, 'name': name, 'geometry': json.dumps(geometry)})\n",
    "\n",
    "    # move to bigquery\n",
    "    table_ref = dataset_ref.table('acs_cbsa_boundaries_%s' % year)\n",
    "    print('%s: setting up table \"%s\"' % (year, table_ref))\n",
    "    bq.delete_table(table_ref, not_found_ok=True)\n",
    "    bq.create_table(table_ref)\n",
    "    job_config = bigquery.LoadJobConfig()\n",
    "    job_config.source_format = bigquery.SourceFormat.CSV\n",
    "    job_config.autodetect = True\n",
    "    with open(table_data_path, \"rb\") as source_file:\n",
    "        print('%s: uploading...' % year)\n",
    "        job = bq.load_table_from_file(source_file, table_ref, job_config=job_config)\n",
    "        print('%s: done' % year)\n",
    "    try:\n",
    "        job.result() # wait for upload to complete\n",
    "    except:\n",
    "        for err in job.errors:\n",
    "            print(err)\n",
    "    return table_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "=> loading 2007\n2007: writing data to \"../_data/tmp/acs_cbsa_2007/bq.csv\"\n2007: setting up table \"TableReference(DatasetReference('eosc410-project', 'data'), 'acs_cbsa_boundaries_2007')\"\n2007: uploading...\n2007: done\n=> loading 2008\n2008: writing data to \"../_data/tmp/acs_cbsa_2008/bq.csv\"\n2008: setting up table \"TableReference(DatasetReference('eosc410-project', 'data'), 'acs_cbsa_boundaries_2008')\"\n2008: uploading...\n2008: done\n=> loading 2009\n2009: writing data to \"../_data/tmp/acs_cbsa_2009/bq.csv\"\n2009: setting up table \"TableReference(DatasetReference('eosc410-project', 'data'), 'acs_cbsa_boundaries_2009')\"\n2009: uploading...\n2009: done\n=> loading 2010\n2010: writing data to \"../_data/tmp/acs_cbsa_2010/bq.csv\"\n2010: setting up table \"TableReference(DatasetReference('eosc410-project', 'data'), 'acs_cbsa_boundaries_2010')\"\n2010: uploading...\n2010: done\n=> loading 2011\n2011: writing data to \"../_data/tmp/acs_cbsa_2011/bq.csv\"\n2011: setting up table \"TableReference(DatasetReference('eosc410-project', 'data'), 'acs_cbsa_boundaries_2011')\"\n2011: uploading...\n2011: done\n=> loading 2012\n2012: writing data to \"../_data/tmp/acs_cbsa_2012/bq.csv\"\n2012: setting up table \"TableReference(DatasetReference('eosc410-project', 'data'), 'acs_cbsa_boundaries_2012')\"\n2012: uploading...\n2012: done\n=> loading 2013\n2013: writing data to \"../_data/tmp/acs_cbsa_2013/bq.csv\"\n2013: setting up table \"TableReference(DatasetReference('eosc410-project', 'data'), 'acs_cbsa_boundaries_2013')\"\n2013: uploading...\n2013: done\n=> loading 2014\n2014: writing data to \"../_data/tmp/acs_cbsa_2014/bq.csv\"\n2014: setting up table \"TableReference(DatasetReference('eosc410-project', 'data'), 'acs_cbsa_boundaries_2014')\"\n2014: uploading...\n2014: done\n=> loading 2015\n2015: writing data to \"../_data/tmp/acs_cbsa_2015/bq.csv\"\n2015: setting up table \"TableReference(DatasetReference('eosc410-project', 'data'), 'acs_cbsa_boundaries_2015')\"\n2015: uploading...\n2015: done\n=> loading 2016\n2016: writing data to \"../_data/tmp/acs_cbsa_2016/bq.csv\"\n2016: setting up table \"TableReference(DatasetReference('eosc410-project', 'data'), 'acs_cbsa_boundaries_2016')\"\n2016: uploading...\n2016: done\n=> loading 2017\n2017: writing data to \"../_data/tmp/acs_cbsa_2017/bq.csv\"\n2017: setting up table \"TableReference(DatasetReference('eosc410-project', 'data'), 'acs_cbsa_boundaries_2017')\"\n2017: uploading...\n2017: done\n=> loading 2018\n2018: writing data to \"../_data/tmp/acs_cbsa_2018/bq.csv\"\n2018: setting up table \"TableReference(DatasetReference('eosc410-project', 'data'), 'acs_cbsa_boundaries_2018')\"\n2018: uploading...\n2018: done\n"
    }
   ],
   "source": [
    "# upload all years\n",
    "for year in acs_years:\n",
    "    upload_year_to_bq(year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all our geometries available in BigQuery, let's attempt the proof-of-concept from before entirely in BigQuery:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Stations in region 27980 \"Kahului-Wailuku, HI\" in 2007:\nsite 9000 (Haleakala National Park, HI 96768, Kahului-Wailuku-Lahaina, HI)\nsite 0006 (KAIHOI ST AND KAIOLOHIA ST, Kahului-Wailuku-Lahaina, HI)\nsite 9001 (Haleakala National Park, Kahului-Wailuku-Lahaina, HI)\n"
    }
   ],
   "source": [
    "resp = bq.query('''\n",
    "    SELECT DISTINCT\n",
    "        epa.site_num, epa.address, epa.cbsa_name\n",
    "    FROM\n",
    "        `eosc410-project.data.epa_air_quality_annual` as epa,\n",
    "        `eosc410-project.data.acs_cbsa_boundaries_%s` as acs\n",
    "    WHERE\n",
    "        ST_WITHIN(ST_GEOGPOINT(epa.longitude, epa.latitude), ST_GEOGFROMGEOJSON(acs.geometry))\n",
    "        AND epa.year = %s\n",
    "        AND acs.geoid = %s\n",
    "''' % (test_year, test_year, test_region_id))\n",
    "\n",
    "print('Stations in region %s \"%s\" in %s:' % (test_region_id, test_region_name, test_year))\n",
    "for row in resp:\n",
    "    print('site %s (%s, %s)' % (row['site_num'], row['address'], row['cbsa_name']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That worked! Now for each year, dump all relationships between EPA stations and ACS regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Utility to match EPA stations with ACS regions for a given year\n",
    "'''\n",
    "def get_relations_for_year(year) -> pd.DataFrame:\n",
    "    print('%s: querying' % year)\n",
    "    start = time.time()\n",
    "    resp = bq.query('''\n",
    "        SELECT DISTINCT\n",
    "            acs.year,\n",
    "            acs.geoid AS acs_geoid,\n",
    "            acs.name AS acs_cbsa_name,\n",
    "            CONCAT(epa.state_code, \".\", epa.county_code, \".\", epa.site_num) AS epa_site,\n",
    "            epa.longitude,\n",
    "            epa.latitude\n",
    "        FROM\n",
    "            `eosc410-project.data.epa_air_quality_annual` as epa,\n",
    "            `eosc410-project.data.acs_cbsa_boundaries_%s` as acs\n",
    "        WHERE\n",
    "            epa.year = %s\n",
    "            AND acs.year = %s\n",
    "            AND ST_WITHIN(ST_GEOGPOINT(epa.longitude, epa.latitude), ST_GEOGFROMGEOJSON(acs.geometry))\n",
    "            AND (epa.datum='NAD83' OR epa.datum='WGS84') # guard against irrelevant coordinates\n",
    "    ''' % (year, year, year))\n",
    "    print('%s: done, took %s' % (year, time.time()-start))\n",
    "    return resp.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "2007: querying\n2007: done, took 0.38341665267944336\n      year  acs_geoid                                 acs_cbsa_name  \\\n0     2007      13620                                 Berlin, NH-VT   \n1     2007      25260                          Hanford-Corcoran, CA   \n2     2007      22660                     Fort Collins-Loveland, CO   \n3     2007      22660                     Fort Collins-Loveland, CO   \n4     2007      33340             Milwaukee-Waukesha-West Allis, WI   \n...    ...        ...                                           ...   \n2680  2007      38060                   Phoenix-Mesa-Scottsdale, AZ   \n2681  2007      33220                                   Midland, MI   \n2682  2007      33100       Miami-Fort Lauderdale-Pompano Beach, FL   \n2683  2007      39740                                   Reading, PA   \n2684  2007      47900  Washington-Arlington-Alexandria, DC-VA-MD-WV   \n\n         epa_site   longitude   latitude  \n0     33.007.4002  -71.217639  44.308132  \n1     06.031.0004 -119.565650  36.102244  \n2     08.069.1004 -105.078920  40.577470  \n3     08.069.0007 -105.545640  40.278130  \n4     55.133.0027  -88.215070  43.020075  \n...           ...         ...        ...  \n2680  04.021.3005 -111.555100  32.755900  \n2681  26.111.0917  -84.193875  43.583083  \n2682  12.011.0011  -80.145833  26.098333  \n2683  42.011.1717  -75.914444  40.377222  \n2684  51.630.0004  -77.487120  38.302250  \n\n[2685 rows x 6 columns]\n"
    }
   ],
   "source": [
    "# test one one year\n",
    "df_2007 = get_relations_for_year('2007')\n",
    "print(df_2007)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'acs_years' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-238c11ded297>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# get for all years and dump each\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0myear\u001b[0m \u001b[0;32min\u001b[0m \u001b[0macs_years\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_relations_for_year\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../_data/epa_to_acs_cbsa/%s.csv'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0myear\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'acs_years' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# get for all years and dump each\n",
    "for year in acs_years:\n",
    "    df = get_relations_for_year(year)\n",
    "    out = '../_data/epa_to_acs_cbsa/%s.csv' % year\n",
    "    if os.path.exists(out):\n",
    "        os.remove(out)\n",
    "    df.to_csv(out)\n",
    "    print('%s: dumped to %s' % (year, out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at what the distribution of EPA stations looks like for 2018!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "=> loading 2018\ndone\n"
    }
   ],
   "source": [
    "# load data\n",
    "year = '2018'\n",
    "stations = pd.read_csv('../_data/epa_to_acs_cbsa/%s.csv' % year)\n",
    "regions = load_geojson(year)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 1920x1080 with 0 Axes>"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "found 408 regions without stations:\n['39700', '39940', '39980', '40100', '40260', '40300', '40460', '40620', '40780', '40820', '41900', '40940', '40980', '41060', '41220', '41660', '41780', '42180', '42740', '42780', '42460', '42620', '42820', '43020', '43060', '43140', '44860', '43180', '43300', '44920', '43380', '44940', '43460', '43500', '43660', '43940', '43980', '44020', '44260', '44340', '44420', '44500', '44540', '44580', '44620', '44660', '44740', '44780', '44980', '45000', '45020', '45180', '45380', '45540', '45580', '45620', '45660', '45700', '45740', '46100', '46460', '46500', '46620', '46740', '46780', '46820', '46900', '46980', '47080', '47420', '47540', '47660', '47700', '48100', '48180', '48460', '49780', '49820', '48660', '48820', '48980', '49080', '49220', '49260', '49300', '49380', '49460', '34660', '35140', '34860', '35020', '35060', '35100', '35420', '35440', '35460', '35500', '35580', '35740', '35820', '35860', '35900', '35940', '36020', '36140', '36340', '36380', '36460', '36580', '36620', '36660', '36700', '36780', '36820', '36840', '36900', '36940', '37020', '37060', '37120', '37300', '37940', '37420', '37500', '37540', '37580', '37660', '37780', '38100', '38220', '38240', '38260', '38380', '38460', '38500', '38700', '38740', '38780', '38920', '39060', '39460', '10180', '10220', '10380', '10460', '10620', '10660', '10700', '10820', '10860', '10940', '10980', '11060', '11180', '11220', '11380', '11420', '11500', '11580', '11680', '11740', '11820', '11860', '19580', '19620', '19760', '19860', '19940', '20140', '20220', '20300', '20340', '20380', '30420', '30580', '30660', '30820', '30880', '30900', '30940', '31260', '31300', '31380', '31500', '31580', '31620', '31680', '31740', '31860', '31900', '31940', '31980', '32000', '32100', '32180', '32260', '32280', '32300', '32340', '32420', '32460', '32500', '32620', '32660', '32700', '32740', '32860', '32980', '33020', '33260', '33300', '33420', '33580', '33620', '34020', '34220', '34260', '34300', '34340', '34380', '34460', '34500', '20660', '20780', '20900', '21020', '21120', '21180', '21300', '21380', '21460', '21540', '21840', '22060', '22260', '22300', '22340', '22580', '22620', '22700', '22780', '22820', '22860', '23140', '23180', '23240', '23300', '23380', '23500', '23620', '23660', '23700', '23780', '23980', '24020', '24100', '24140', '24220', '24380', '24460', '24620', '24700', '24740', '24820', '24900', '24940', '25100', '25300', '25580', '25700', '25720', '25760', '25780', '25820', '25880', '25940', '25980', '26220', '26460', '26500', '26660', '26700', '26740', '26780', '26940', '27020', '27100', '27160', '27300', '27340', '27380', '27420', '27580', '27600', '27700', '27740', '27920', '28100', '28260', '28340', '28380', '28500', '28540', '28580', '28620', '28740', '28860', '29300', '29500', '29780', '29860', '29900', '29940', '30060', '30220', '30260', '30280', '30380', '11980', '12140', '12180', '12220', '12380', '12460', '12680', '12740', '12820', '12860', '12900', '12980', '13060', '13100', '13220', '13260', '13300', '13340', '13500', '13660', '13720', '13780', '13940', '14100', '14140', '14180', '14220', '14340', '14380', '14620', '14720', '14780', '14820', '15020', '15060', '15220', '15340', '15420', '15460', '15500', '15660', '15680', '15740', '15780', '15820', '15900', '16020', '16060', '16340', '16380', '16460', '16660', '17060', '17260', '17420', '17500', '17580', '17620', '17640', '17700', '17740', '18060', '18100', '18220', '18300', '18380', '18420', '18460', '18660', '18740', '18820', '18900', '18980', '19180', '19220', '19260', '19420', '19540', '10760', '14300', '15140', '16140', '16420', '21640', '21860', '24330', '26260', '27660', '37800', '48500']\n"
    }
   ],
   "source": [
    "# random rgb value generator\n",
    "r = lambda: random.randint(0,255)\n",
    "\n",
    "# just good to knnow\n",
    "no_stations = []\n",
    "\n",
    "# set up plot\n",
    "print('starting')\n",
    "dpi=72\n",
    "plt.figure(figsize=(1920/dpi,1080/dpi),dpi=dpi)\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(len(regions)):\n",
    "    # check if this region has any stations\n",
    "    geoid = regions.loc[i]['CBSAFP']\n",
    "    sts = stations[stations.acs_geoid==int(geoid)]\n",
    "    if sts.size > 0:\n",
    "        # random color for this region\n",
    "        c = '#%02x%02x%02x' % (r(), r(), r())\n",
    "\n",
    "        # plot region\n",
    "        # regions.loc[[i],'geometry'].plot(ax=ax, color='white', edgecolor=c)\n",
    "\n",
    "        # plot stations\n",
    "        gdf = gp.GeoDataFrame(sts, geometry=gp.points_from_xy(sts.longitude, sts.latitude))\n",
    "        gdf.plot(ax=ax, marker='o', color=c, markersize=5)\n",
    "    else:\n",
    "        print('%d: no stations in region %s, ignoring' % (i, geoid))\n",
    "        no_stations.append(geoid)\n",
    "\n",
    "print('wrapping up')\n",
    "plt.title('ACS CBSA Regions and EPA Stations in %s' % year)\n",
    "plt.tight_layout()\n",
    "plt.savefig('./figs/2/epa_stations_in_acs_regions_2018.png')\n",
    "\n",
    "print('found %d regions without stations:' % len(no_stations))\n",
    "print(no_stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}