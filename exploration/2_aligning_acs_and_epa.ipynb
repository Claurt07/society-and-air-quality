{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37464bitanaconda3virtualenv214083d9e99e48dbbb65188af8bf868a",
   "display_name": "Python 3.7.4 64-bit ('anaconda3': virtualenv)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Aligning ACS and EPA Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our historical air quality data from the EPA is provided station by station, and the positions of each station is provided by coordinates. To make useful comparisons, we want to align the data - decide for each year which exact air quality data points align with each CBSA GeoID/region in ACS.\n",
    "\n",
    "We'll refer to these as `epa_point` and `acs_region` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "env: GOOGLE_APPLICATION_CREDENTIALS=../google_app_credentials.json\n"
    }
   ],
   "source": [
    "# set up path to app credentials - see exploration/README.md\n",
    "%env GOOGLE_APPLICATION_CREDENTIALS=../google_app_credentials.json\n",
    "\n",
    "# set up bigquery client\n",
    "from google.cloud import bigquery\n",
    "bq = bigquery.Client()\n",
    "\n",
    "# set up some parameters\n",
    "BQ_DATASET_ID = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up some dependencies\n",
    "import geopandas as gp\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll load up the relevant data, which should already be set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# set up acs utils\n",
    "resp = bq.query('''\n",
    "    SELECT DISTINCT do_date AS year\n",
    "    FROM `eosc410-project.data.acs_cbsa_20*`\n",
    "    ORDER BY do_date ASC\n",
    "''')\n",
    "acs_years = [row[\"year\"] for row in resp]\n",
    "\n",
    "def load_geojson(y) -> gp.GeoDataFrame:\n",
    "    print('=> loading %s' % y)\n",
    "    geo = gp.read_file('../_data/tmp/acs_cbsa_%s/geojson.json' % y)\n",
    "    return geo\n",
    "\n",
    "def get_acs_year(i: int) -> str:\n",
    "    return str(int(acs_years[0]) + i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "=> loading 2007\n2007 has 952 features (regions)\n27980: Kahului-Wailuku, HI\n{\"type\": \"MultiPolygon\", \"coordinates\": [[[[-156.70540599999998, 20.82632], [-156.711227, 20.832228]...\n"
    }
   ],
   "source": [
    "# look at what our geojson looks like\n",
    "test_year = get_acs_year(0)\n",
    "test_region_geojson = load_geojson(test_year)\n",
    "print('%s has %d features (regions)' % (test_year, len(test_region_geojson)))\n",
    "\n",
    "test_region = test_region_geojson.loc[0] # first region in this year\n",
    "test_region_id = test_region['CBSAFP']   # current metropolitan statistical area/micropolitan statistical area code\n",
    "                                         # see https://www2.census.gov/geo/pdfs/maps-data/data/tiger/tgrshp2014/TGRSHP2014_TechDoc_Ch3.pdf\n",
    "test_region_name = test_region['NAME']   # this region has a name\n",
    "test_region_geo = test_region.geometry   # this region has a geometry\n",
    "print('%s: %s' % (test_region_id, test_region_name))\n",
    "print(json.dumps(test_region_geo.__geo_interface__)[:100] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Stations in region 27980 \"Kahului-Wailuku, HI\" in 2007:\nsite 9001 (Haleakala National Park, Kahului-Wailuku-Lahaina, HI)\nsite 0006 (KAIHOI ST AND KAIOLOHIA ST, Kahului-Wailuku-Lahaina, HI)\nsite 9000 (Haleakala National Park, HI 96768, Kahului-Wailuku-Lahaina, HI)\n"
    }
   ],
   "source": [
    "# for each year and acs region, we want to get associated station numbers\n",
    "# try with the test region: find all EPA points that fall within this region's geometry in that year\n",
    "resp = bq.query('''\n",
    "SELECT DISTINCT site_num, address, cbsa_name\n",
    "FROM `eosc410-project.data.epa_air_quality_annual` as epa\n",
    "WHERE\n",
    "  ST_WITHIN(ST_GEOGPOINT(epa.longitude, epa.latitude), ST_GEOGFROMGEOJSON('%s'))\n",
    "  AND year = %s\n",
    "''' % (json.dumps(test_region_geo.__geo_interface__), test_year))\n",
    "print('Stations in region %s \"%s\" in %s:' % (test_region_id, test_region_name, test_year))\n",
    "for row in resp:\n",
    "    print('site %s (%s, %s)' % (row['site_num'], row['address'], row['cbsa_name']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As demonstrated in the proof of concept above, we can easily query for geographic traits in BigQuery. Let's load the ACS geometries into BigQuery directly, since once we compile the regions we can avoid handling massive geometry data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "'''\n",
    "A utility function to upload a year of boundaries to BigQuery\n",
    "'''\n",
    "dataset_ref = bq.dataset(BQ_DATASET_ID)\n",
    "def upload_year_to_bq(year):\n",
    "    # convert to a nice table format\n",
    "    table_data_path = '../_data/tmp/acs_cbsa_%s/bq.csv' % year\n",
    "    print('%s: writing data to \"%s\"' % (year, table_data_path))\n",
    "    if os.path.exists(table_data_path):\n",
    "        os.remove(table_data_path)\n",
    "    geo = load_geojson(year)\n",
    "    with open(table_data_path, 'w') as csvfile:\n",
    "        w = csv.DictWriter(csvfile, fieldnames=['geoid', 'name', 'geometry'])\n",
    "        w.writeheader()\n",
    "        for i in range(0, len(geo)):\n",
    "            region = geo.loc[i]\n",
    "            geoid = region['CBSAFP']\n",
    "            name = region['NAME']\n",
    "            geometry = region.geometry.__geo_interface__\n",
    "            w.writerow({'geoid': geoid, 'name': name, 'geometry': json.dumps(geometry)})\n",
    "\n",
    "    # move to bigquery\n",
    "    table_ref = dataset_ref.table('acs_cbsa_boundaries_%s' % year)\n",
    "    print('%s: setting up table \"%s\"' % (year, table_ref))\n",
    "    bq.delete_table(table_ref, not_found_ok=True)\n",
    "    bq.create_table(table_ref)\n",
    "    job_config = bigquery.LoadJobConfig()\n",
    "    job_config.source_format = bigquery.SourceFormat.CSV\n",
    "    job_config.autodetect = True\n",
    "    with open(table_data_path, \"rb\") as source_file:\n",
    "        print('%s: uploading...' % year)\n",
    "        job = bq.load_table_from_file(source_file, table_ref, job_config=job_config)\n",
    "        print('%s: done' % year)\n",
    "    try:\n",
    "        job.result() # wait for upload to complete\n",
    "    except:\n",
    "        for err in job.errors:\n",
    "            print(err)\n",
    "    return table_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "2007: writing data to \"../_data/tmp/acs_cbsa_2007/bq.csv\"\n=> loading 2007\n2007: setting up table \"TableReference(DatasetReference('eosc410-project', 'data'), 'acs_cbsa_boundaries_2007')\"\n2007: uploading...\n2007: done\n"
    }
   ],
   "source": [
    "# trial the function on one year\n",
    "table_ref = upload_year_to_bq(get_acs_year(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload all years\n",
    "# TODO"
   ]
  }
 ]
}